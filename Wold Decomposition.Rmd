---
title: "Wold's Decomposition"
author: "Carlos Pequeño (aka Pareto Deficient)"
output: html_document
---


```{julia, results='hide'}
using Random
using GLM
using Statistics
```

## Introduction

This document explains Wold's decomposition and illustrates its application in time series analysis, as well as its limitations when the actual data-generating process (DGP) is not stationary (e.g. Random Walk) or exhibits high nonlinearities.

## Setup

Consider the following equation:

$$
y(t) =  u(t) + 0.2 \cdot u(t-1) + 0.8 \cdot u(t-1) \cdot u(t-2)
$$

which represents the true Data Generating Process (DGP). In practice, especially outside of the simplified classroom models, the actual DGP is typically unknown, and it must be estimated using various techniques. In this notebook, I will apply Wold’s decomposition to analyze the time series.

Wold’s decomposition is a fundamental concept in time series analysis, which decomposes a time series into two components: a predictable (deterministic) part and a stochastic (random) part. This decomposition is particularly useful for understanding the underlying structure of a time series, especially when the true DGP is not known.

This setup can be configured in Julia as follows:

```{julia, results='hide'}
Random.seed!(123) # Seed for reproducibility.

n = 1000
u = randn(n) # As usual, the error is assumed to be normal.
y = u .+ 0.2 .* circshift(u, 1) .+ 0.8 .* circshift(u, 1) .* circshift(u, 2)
```

where the command 'circshift(y, i)' shifts the values of the array y by i positions, e.g., for y = [1, 2, 3], circshift(y, 1) = [2, 3, NaN].

### Best Linear Predictor (BLP)

By the Linear projection theorem, the BLP (assuming the DGP is known) can be computed as:

$$
\beta = (X_{best} X_{best}^\top)^{-1} X_{best} y
$$
where the three regressors above,  $(1, \cdot u(t-1), and \cdot u(t-1) \cdot u(t-2))$, are merged into a single matrix called $X_{best}$. Thus, I run OLS of y on $X_{best}$. In addition to the BLP, I also get the sum of squared residuals, i.e.,

$$
\text{SSR} = \sum_{t=1}^n \left( y_t - \hat{y}_t \right)^2
$$
in order to compute later the variance of the errors The reason is that, later, I will compare it with the variance of the residuals of the Wold's decomposition (and also the variance of the residuals of the AR model I estimate below). In Julia, this is done running the following chunk:

```{julia}
X_best = hcat(circshift(u, 1), circshift(u, 1) .* circshift(u, 2))
best_model = lm(X_best, y)
best_ssr = deviance(best_model)
```

### Theoretical Representation

Formally speaking, the Wold’s decomposition represents a time series as:

$$
y(t) = \mu(t) + \epsilon(t)
$$

where $\mu(t)$ is an autoregressive (AR) process, i.e., the deterministic part, and $\epsilon(t)$ is a moving average (MA) process, i.e., the stochastic part.

To determine the optimal number of lags for the AR model, I should perform model selection by fitting AR models with varying lag lengths (e.g., AR(1), AR(2), ..., AR(n)) and selecting the best model based on a performance metric, such as AIC or BIC. Although I will explore model selection in more detail in subsequent notebooks, for now, I will bypass this step and choose an AR(10), which corresponds to an AR model with 10 lags.

```{julia, results='hide'}
lags = 10
```

Why an AR(10)? Good question, my (probably) non-existent reader. But, if truth be told, the choice is somewhat arbitrary. The idea is to capture enough of the autocorrelation in the data. For a process like the one I dealing with, choosing a higher number of lags (such as 10) can help capture long-term dependencies as the behavior of the series may depend on multiple previous time periods.

To estimate the deterministic part, $\mu(t)$, I create the following lagged matrix \(X_{ary}\):

```{julia}
X_ary = hcat([circshift(y, i) for i in 1:lags]...)
```

This code looks a bit weird, doesn't it? Let me explain it by parts, like Jack the Ripper:

First of all,'[circshift(y, i) for i in 1:lags]' creates the following array:

$$
[circshift(y, 1), circshift(y, 2), \ldots, circshift(y, 10)]
$$
where, as I explained before, 'circshift(y, i)' lags the time series by i periods.

Second, 'hcat([circshift(y, i) for i in 1:lags])' horizontally concatenates arrays. Thus, a list of ten elements where each element is $circshift(y, i)$ for $i \in {1, 2, ..., 10}$ is obtained. However, for regression purposes, a matrix rather than a vector is required. That is the reason for the three dots after the squared brackets, called the 'splat operator'. This splat operator is used to "unpack" the list of arrays so that 'hcat' receives them as separate arguments.

For illustration, let $y = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]$. Similar to the actual example, suppose we need to compute 10 lags of this process. In this case, the expression hcat([circshift(y, i) for i in 1:lags]...) generates the following **matrix**:

$$
X_{ary} = 
\begin{bmatrix}
2  & 3  & \ldots & 10 & 11 \\
3  & 4  & \ldots & 11 & 12 \\
4  & 5  & \ldots & 12 & \text{NaN} \\
5  & 6  & \ldots & \text{NaN} & \text{NaN} \\
6  & 7  & \ldots & \text{NaN} & \text{NaN} \\
7  & 8  & \ldots & \text{NaN} & \text{NaN} \\
8  & 9  & \ldots & \text{NaN} & \text{NaN} \\
9  & 10 & \ldots & \text{NaN} & \text{NaN} \\
10 & 11 & \ldots & \text{NaN} & \text{NaN} \\
11 & 12 & \ldots & \text{NaN} & \text{NaN} \\
12 & \text{NaN} & \ldots & \text{NaN} & \text{NaN} 
\end{bmatrix}
$$
while 'hcat([circshift(y, i) for i in 1:lags])' (note here the absence of the splat operator, i.e., the three dots) gives the following **list**:

$$
X_{ary} = 
\begin{bmatrix}
[2, 3, \ldots, 10, 11] \\
[3, 4, \ldots, 11, 12] \\
[4, 5, \ldots, 12, \text{NaN}] \\
[5, 6, \ldots, \text{NaN}, \text{NaN}] \\
[6, 7, \ldots, \text{NaN}, \text{NaN}] \\
[7, 8, \ldots, \text{NaN}, \text{NaN}] \\
[8, 9, \ldots, \text{NaN}, \text{NaN}] \\
[9, 10, \ldots, \text{NaN}, \text{NaN}] \\
[10, 11, \ldots, \text{NaN}, \text{NaN}] \\
[11, 12, \ldots, \text{NaN}, \text{NaN}]
\end{bmatrix}
$$
### Estimating the AR model

With this understood, let me continue and estimate the AR(10) process, i.e.,

$$
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \phi_3 y_{t-3} + \cdots + \phi_{10} y_{t-10} + \epsilon_t
$$

and, in a similar fashion as before, obtain the sum of the squared residuals.

```{julia}
ary_model = lm(X_ary, y)
ary_ssr = deviance(ary_model)
```


### Estimating the MA model

Let me now to estimate the non-deterministic part of the Wold's decomposition. The residuals from the regression above, $\epsilon_t$, represent this stochastic component.

```{julia, results='hide'}
residuals_ary = residuals(ary_model)
```

With them, I estimate a MA(30) and obtain the sum of squared residuals. Mathematically, a MA(30) takes the following form:

$$
y_t = \theta_0 + \sum_{i=1}^{30} \theta_i e_{t-i} + e_t
$$
which, in Julia, can be computed in a similar fashion as before:

```{julia}
lags_residuals = 30
X_wold = hcat([circshift(residuals_ary, i) for i in 1:lags_residuals]...)
wold_model = lm(X_wold, y)
wold_ssr = deviance(wold_model)
```

Using 30 lags is again arbitrary, but it allows to capture dependencies in the stochastic process up to a certain point. In practice, the number of lags would again depend on model selection techniques or the characteristics of the data.


### Comparison of Models

Finally, I compare the variance of the residuals across each approach: the model assuming knowledge of the true DGP, the AR(10) model, and Wold's decomposition

$$
\text{Var}_{\text{best}} = \frac{\text{SSR}_{\text{best}}}{n}
$$
```{julia}
var_best = best_ssr / length(y)
```


$$
\text{Var}_{\text{ary}} = \frac{\text{SSR}_{\text{ary}}}{n}
$$

```{julia}
var_ary = ary_ssr / length(y)
```


$$
\text{Var}_{\text{wold}} = \frac{\text{SSR}_{\text{wold}}}{n}
$$

```{julia}
var_wold = wold_ssr / length(y)
```


The results are striking. When the DGP is known, the variance of the errors is 0.97, while the variances of the residuals for the AR(10) model and Wold's decomposition are 1.52 and 1.50, respectively. Notably, Wold's decomposition performs exceptionally well. However, this is not the end of the story.


### Limitations of Wold's Decomposition

The performance of Wold's decomposition significantly deteriorates when the model involves high nonlinearities or is a non-stationary process, such as a random walk. For instance, consider the case where the actual DGP is

$$
x_t = x_{t-1} + u_t
$$

```{julia}
y = cumsum(u)
```

which is a random walk. The best predictor for this model is

$$
\hat{x}_t = \mathbb{E}[x_t | \mathcal{F}_{t-1}] = x_{t-1}
$$

```{julia}
X_best = hcat(circshift(y, 1))
```

In such cases, the residual variance of Wold's decomposition becomes excessively high. Specifically, while the variance of the residuals under the assumption of knowing the true DGP is approximately 3.55, the variance of Wold's decomposition skyrockets to 753.80. To verify this, re-run the entire notebook, replacing the original DGP with the new one and updating the BP accordingly.

